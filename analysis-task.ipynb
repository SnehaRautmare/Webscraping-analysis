{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task\n",
    "Find attached html pages and perform interesting tasks on it\n",
    "- Extract all links from all pages and present them in a structured format (e.g. print a JSON document with all links)\n",
    "- Download all images to a folder and print interesting statistics to get some insights (e.g. image width/height and size)\n",
    "- Try to find the frequency of different word categories (nouns, adjectives,...) across all pages.\n",
    "- Analyze the content in any other interesting way. Skillful visualization of your analysis is a plus!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Link extraction\n",
    "### Extract all links from all pages and present them in a structured format (e.g. print a JSON document with all links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in /data/anaconda/envs/py36/lib/python3.6/site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /data/anaconda/envs/py36/lib/python3.6/site-packages (from bs4) (4.7.1)\n",
      "Requirement already satisfied: soupsieve>=1.2 in /data/anaconda/envs/py36/lib/python3.6/site-packages (from beautifulsoup4->bs4) (1.9.1)\n",
      "Requirement already satisfied: nltk in /data/anaconda/envs/py36/lib/python3.6/site-packages (3.4.3)\n",
      "Requirement already satisfied: six in /data/anaconda/envs/py36/lib/python3.6/site-packages (from nltk) (1.12.0)\n",
      "Requirement already satisfied: plotly in /data/anaconda/envs/py36/lib/python3.6/site-packages (3.10.0)\n",
      "Requirement already satisfied: six in /data/anaconda/envs/py36/lib/python3.6/site-packages (from plotly) (1.12.0)\n",
      "Requirement already satisfied: nbformat>=4.2 in /data/anaconda/envs/py36/lib/python3.6/site-packages (from plotly) (4.4.0)\n",
      "Requirement already satisfied: decorator>=4.0.6 in /data/anaconda/envs/py36/lib/python3.6/site-packages (from plotly) (4.4.0)\n",
      "Requirement already satisfied: retrying>=1.3.3 in /data/anaconda/envs/py36/lib/python3.6/site-packages (from plotly) (1.3.3)\n",
      "Requirement already satisfied: requests in /data/anaconda/envs/py36/lib/python3.6/site-packages (from plotly) (2.21.0)\n",
      "Requirement already satisfied: pytz in /data/anaconda/envs/py36/lib/python3.6/site-packages (from plotly) (2019.1)\n",
      "Requirement already satisfied: ipython_genutils in /data/anaconda/envs/py36/lib/python3.6/site-packages (from nbformat>=4.2->plotly) (0.2.0)\n",
      "Requirement already satisfied: traitlets>=4.1 in /data/anaconda/envs/py36/lib/python3.6/site-packages (from nbformat>=4.2->plotly) (4.3.2)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /data/anaconda/envs/py36/lib/python3.6/site-packages (from nbformat>=4.2->plotly) (3.0.1)\n",
      "Requirement already satisfied: jupyter_core in /data/anaconda/envs/py36/lib/python3.6/site-packages (from nbformat>=4.2->plotly) (4.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /data/anaconda/envs/py36/lib/python3.6/site-packages (from requests->plotly) (2019.3.9)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /data/anaconda/envs/py36/lib/python3.6/site-packages (from requests->plotly) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /data/anaconda/envs/py36/lib/python3.6/site-packages (from requests->plotly) (1.24.3)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /data/anaconda/envs/py36/lib/python3.6/site-packages (from requests->plotly) (2.8)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /data/anaconda/envs/py36/lib/python3.6/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2->plotly) (19.1.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /data/anaconda/envs/py36/lib/python3.6/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2->plotly) (0.14.11)\n",
      "Requirement already satisfied: setuptools in /data/anaconda/envs/py36/lib/python3.6/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2->plotly) (41.0.1)\n"
     ]
    }
   ],
   "source": [
    "#List of required packages and installation\n",
    "!pip install bs4\n",
    "!pip install nltk\n",
    "!pip install plotly\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import os\n",
    "import html2text\n",
    "import plotly.plotly as py\n",
    "import numpy as np\n",
    "import plotly.tools as tls\n",
    "import nltk\n",
    "tls.set_credentials_file(username= 'snehar', api_key = \"e96QTBepc3ihn8qB3Ry4\")\n",
    "import plotly.graph_objs as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_links(webpage_path):\n",
    "    '''\n",
    "    The Function find_links accept the address of webpage present in the directory\n",
    "    and return all the hyperlinks present in the source code of the HTML.\n",
    "    \n",
    "    The Function uses beautifulsoup library for parsing the HTML document.\n",
    "    '''\n",
    "    # Open the HTML document, present in webpage_path and opens it in read mode\n",
    "    f = open(webpage_path,'r')\n",
    "    # Use BeautifulSoup as HTML Parser i.e. it understands the HTML tags and helps in navigation\n",
    "    soup = BeautifulSoup(f, 'html.parser')\n",
    "    # Created the empty list that will contains the webpage hyperlinks\n",
    "    webpage_hyperlinks = []\n",
    "    # The hyperlink can be in 'a' tag as well as 'link' tag. Therefore, 2 for loops are used to search the links for each tags.\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        webpage_hyperlinks.append(link.get('href'))\n",
    "    for link in soup.find_all('link', href=True):\n",
    "        webpage_hyperlinks.append(link.get('href'))\n",
    "    # There can be repetative links in the list. Therefore, it is important to get unique hyperlink that can be done using set.\n",
    "    webpage_hyperlinks = set(webpage_hyperlinks)\n",
    "    print(\"Total Links in \" +  webpage_path + ' is : ' + str(len(webpage_hyperlinks)))\n",
    "    return webpage_hyperlinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def webcontent_sentiment_analysis(address):\n",
    "    '''\n",
    "    The function 'webcontent_sentiment_analysis' convert the html to txt and then using nltk package find the sentiment of the content.\n",
    "    '''\n",
    "    html = open(address).read()\n",
    "    text = html2text.html2text(html)\n",
    "\n",
    "    # nltk.download('vader_lexicon')\n",
    "    # nltk.download('popular')\n",
    "    from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    sentiment_summary = {\"positive\":0,\"neutral\":0,\"negative\":0}\n",
    "    text = text.split('\\n')\n",
    "    for readme in text:\n",
    "        sentences = nltk.tokenize.sent_tokenize(readme)\n",
    "        for sentence in sentences:\n",
    "\n",
    "            sentiment_score = sid.polarity_scores(sentence)\n",
    "            if sentiment_score[\"compound\"] == 0.0:\n",
    "                sentiment_summary[\"neutral\"] += 1\n",
    "            elif sentiment_score[\"compound\"] > 0.0:\n",
    "                sentiment_summary[\"positive\"] += 1\n",
    "            else:\n",
    "                sentiment_summary[\"negative\"] += 1\n",
    "    return sentiment_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Links in ./wikipedia/wikipedia/Richard_Stallman.html is : 1429\n",
      "Total Links in ./wikipedia/wikipedia/Konrad_Zuse.html is : 546\n",
      "Total Links in ./wikipedia/wikipedia/Blaise_Pascal.html is : 1849\n",
      "Total Links in ./wikipedia/wikipedia/Alan_Turing.html is : 1870\n",
      "Total Links in ./wikipedia/wikipedia/Donal_Knuth.html is : 1662\n",
      "Total Links in ./wikipedia/wikipedia/Grace_Hopper.html is : 1388\n",
      "Total Links in ./wikipedia/wikipedia/Ada_Lovelace.html is : 1025\n"
     ]
    }
   ],
   "source": [
    "# The is the main function, where code execution will start\n",
    "# Requirements: Please place the folder in the same structure as provided.\n",
    "# If you want to change the strcture, then provide similar path in 2 lines: i.e. listdir() , and join()\n",
    "webpage_links = []\n",
    "webpage_data = {}\n",
    "file_names = []\n",
    "webpages_sentiment = []\n",
    "# The for loop will extract all the webpages according to the folder structure and call 'find_links' function accordingly.\n",
    "# The function return the set and using the Python's Json package, the data is converted to JSON format.\n",
    "for file in os.listdir(\"./wikipedia/wikipedia\"):\n",
    "    if file.endswith(\".html\"):\n",
    "        file_names.append(file)\n",
    "        address_name = os.path.join(\"./wikipedia/wikipedia\", file)\n",
    "        webpage_links = list(find_links(address_name))\n",
    "        page = {'webpage_address':address_name,'links':webpage_links}\n",
    "        webpage_data[file] = page\n",
    "\n",
    "# Convert the Python Dictonary to JSON format\n",
    "with open('webpages_link_data.json', 'w', encoding='utf-8') as outfile:\n",
    "    json.dump(webpage_data, outfile, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Sentiment Analysis \n",
    "### Sentiment Analysis of website content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High five! You successfully sent some data to your account on plotly. View your plot in your browser at https://plot.ly/~snehar/0 or inside your plot.ly account where it is named 'grouped-bar'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~snehar/0.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<chart_studio.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The Code will do the sentiment analysis of the webpages.\n",
    "file_names = []\n",
    "webpages_sentiment = []\n",
    "# The for loop will extract all the webpages according to the folder structure and call 'webcontent_sentiment_analysis' function accordingly.\n",
    "for file in os.listdir(\"./wikipedia/wikipedia\"):\n",
    "    if file.endswith(\".html\"):\n",
    "        file_names.append(file)\n",
    "        address_name = os.path.join(\"./wikipedia/wikipedia\", file)\n",
    "        # Sentiment Analysis\n",
    "        val = webcontent_sentiment_analysis(address_name).values()\n",
    "        val = list(val)\n",
    "        # Normalize the value \n",
    "        total_val = val[0] + val[1] + val[2]\n",
    "        val[0] = val[0]/total_val\n",
    "        val[1] = val[1]/total_val\n",
    "        val[2] = val[2]/total_val\n",
    "        webpages_sentiment.append(list(val))\n",
    "        \n",
    "traces=[]\n",
    "for n_page in range(0,len(file_names)):\n",
    "    traces.append(\n",
    "        go.Bar(\n",
    "            x=['Positive', 'Neutral', 'Negative'],\n",
    "            y=webpages_sentiment[n_page],\n",
    "            name=file_names[n_page])\n",
    "    )\n",
    "\n",
    "plot_data = traces\n",
    "layout = go.Layout(\n",
    "barmode='group'\n",
    ")\n",
    "fig = go.Figure(data=plot_data, layout=layout)\n",
    "py.iplot(fig, filename='grouped-bar')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Sentiment Analysis Result:\n",
    "\n",
    "### 1. Grace Hopper has highest positive sentiment content.\n",
    "### 2. Alan Turing has highest negative sentiment content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
